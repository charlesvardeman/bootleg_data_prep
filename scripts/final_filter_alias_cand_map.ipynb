{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load alias map to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.symbols.entity_profle import EntityProfile\n",
    "root_dir = Path(\"train_data_dir\")\n",
    "entity_dump = EntityProfile.load_from_cache(load_dir=root_dir / \"entity_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_aliases = entity_dump._entity_symbols.get_alias2qids_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load count files for all of wikipedia --- these were computed with `compute_statistics.py` (in utils/preprocessing) over the merged data file of test, dev, and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times alias phrase occurs in the text across ALL of wikipedia\n",
    "alias_text_counts = ujson.load(\n",
    "    open(root_dir / 'stats/alias_text_counts.json'))\n",
    "\n",
    "# number of times alias occurs as an alias across ALL of wikipedia\n",
    "alias_counts = ujson.load(\n",
    "    open(root_dir / 'stats/alias_counts.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to find aliases to remove based on the count files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_value(alias, verbose=False):\n",
    "    if verbose:\n",
    "        print('# times occurs as alias:', alias_counts.get(alias, 0))\n",
    "        print('# times occurs in text:', alias_text_counts.get(alias, 0))\n",
    "    return alias_counts.get(alias, 0) / (alias_text_counts[alias]) if alias in alias_text_counts else -1\n",
    "\n",
    "\n",
    "def get_aliases_to_remove(curr_aliases, keep_wikidata=False, norm_threshold=0.017, min_seen=500, min_alias_count=10000):\n",
    "    \"\"\"\n",
    "    Remove aliases which are frequent words but infrequent aliases due to rarity \n",
    "    or mislabel (e.g. band \"themselves\").\n",
    "    \"\"\"\n",
    "    aliases_to_remove = set()\n",
    "    cnts = defaultdict(int)\n",
    "    grps = defaultdict(list)\n",
    "    for alias in tqdm(curr_aliases):\n",
    "        # If alias is not seen in Wikipedia\n",
    "        if alias not in alias_counts:\n",
    "            # If alias is seen in text but only a few times, skip as it's too few to make a decision\n",
    "            if (alias in alias_text_counts and alias_text_counts[alias] < min_seen):\n",
    "                continue\n",
    "            # if alias occurs in Wikidata (so it's in our alias map), but not as alias in Wikipedia\n",
    "            # and occurs more than min_seen times, only keep if one candidate (indicating a fairly unique alias)\n",
    "            # and if that one candidate is a type we care about (e.g., people and locations)\n",
    "            elif len(curr_aliases[alias]) == 1:\n",
    "                continue\n",
    "            # else make sure we don't think it's a person or location name - we want to keep those\n",
    "            # even if more general alias\n",
    "            else:\n",
    "                if keep_wikidata:\n",
    "                    continue\n",
    "                cnts[\"not_in_wikipedia\"] += 1\n",
    "                grps[\"not_in_wikipedia\"].append(alias)\n",
    "                aliases_to_remove.add(alias)\n",
    "                continue \n",
    "        # length greater than max_alias_len and weak labels cause some aliases to occur as aliases \n",
    "        # but not occur in the text\n",
    "        if alias not in alias_text_counts:\n",
    "            continue \n",
    "        # filter out aliases which occur commonly in the text but uncommonly as an alias\n",
    "        # we require that the alias is a common phrase in text \n",
    "        # and that the phrase isn't very commonly an alias \n",
    "        if (get_norm_value(alias) < norm_threshold):\n",
    "            if alias_text_counts[alias] > min_seen:\n",
    "                if alias_counts[alias] < min_alias_count:\n",
    "                    aliases_to_remove.add(alias)\n",
    "                    cnts[\"removed_filter\"] += 1\n",
    "                    grps[\"removed_filter\"].append(alias)\n",
    "                else:\n",
    "                    cnts[\"grt_min_alias_cnt\"] += 1\n",
    "                    grps[\"grt_min_alias_cnt\"].append(alias)\n",
    "            else:\n",
    "                cnts[\"lt_min_seen\"] += 1\n",
    "                grps[\"lt_min_seen\"].append(alias)\n",
    "    \n",
    "    return aliases_to_remove, cnts, grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26217/15290555 [00:00<00:58, 262167.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stats to filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15290555/15290555 [00:29<00:00, 521540.39it/s]\n",
      "  0%|          | 57444/15290555 [00:00<00:26, 574436.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILT REM 89363\n",
      "{\n",
      "    \"removed_filter\": 45632,\n",
      "    \"lt_min_seen\": 54010,\n",
      "    \"grt_min_alias_cnt\": 3,\n",
      "    \"not_in_wikipedia\": 43731\n",
      "}\n",
      "Using Wikidata to filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15290555/15290555 [00:19<00:00, 784472.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKI REM 8024947\n",
      "Will remove 8081077 out of 15290555\n"
     ]
    }
   ],
   "source": [
    "aliases_to_remove, cnts, grps = get_aliases_to_remove(curr_aliases)\n",
    "print(ujson.dumps(cnts, indent=4))\n",
    "print(f\"Will remove {len(aliases_to_remove)} out of {len(curr_aliases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks on the filter step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "цезиас мец\n",
      "10 xronia mazi\n",
      "rockfunk\n",
      "ustajikistan relations\n",
      "ak47su\n",
      "body transistor\n",
      "gare dolten\n",
      "henry richardson cricketer born 1846\n",
      "justice william o douglas\n",
      "woollcott alexander\n",
      "mtv swedish tv channel\n",
      "sir michael atiyah\n",
      "west los angeles ca\n",
      "kfar sava\n",
      "hunters island\n",
      "french concession of shanghai\n",
      "roadshow films\n",
      "2015 yale bulldogs football\n",
      "black sea region turkey\n",
      "henry gilroy baseball\n",
      "beckham putra nugraha\n",
      "korbr\n",
      "anne dormer lady hungerford\n",
      "saint torpes of pisa\n",
      "iso 639zir\n",
      "ballets by marius petipa\n",
      "marine cadets\n",
      "pacific northwest bell telephone company\n",
      "avid d weinberger\n",
      "5 x 5 cube\n",
      "draftzayn africa\n",
      "daphne anne caruana galizia\n",
      "national bishop for torres strait people\n",
      "catchment water\n",
      "notre dame fighting irish football 1985\n",
      "lockheed hudson iva\n",
      "englishborn\n",
      "united statesman\n",
      "gadaræ\n",
      "trygve martin bratteli\n",
      "jakobstadt\n",
      "albanian national liberation front\n",
      "sirkesh\n",
      "cuisine of boston\n",
      "still standing tv series\n",
      "norske skogindustrier asa\n",
      "klein charles\n",
      "mpeg2 layer ii\n",
      "poaching of white rhinoceroses\n",
      "the henegar center for the performing arts\n"
     ]
    }
   ],
   "source": [
    "# sample what aliases are getting removed\n",
    "num_to_sample = 50\n",
    "for alias in np.random.choice(list(aliases_to_remove), num_to_sample): \n",
    "    print(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existence of certain words in aliases_to_remove\n",
    "sanity_checks = [('themselves', True), \n",
    "                 ('dolittle', False),\n",
    "                 ('us', False),\n",
    "                 ('s', True),\n",
    "                 ('is', True),\n",
    "                 ('also', True),\n",
    "                 ('in a world', True), \n",
    "                 ('of', True),\n",
    "                 ('the', True),\n",
    "                 ('by year', True),\n",
    "                 ('apoptosis', False),\n",
    "                 ('england', False)]\n",
    "for s, bool_val in sanity_checks: \n",
    "    assert (s in aliases_to_remove) is bool_val, f'{s} {bool_val} {s in aliases_to_remove}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove aliases and save new candidate mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading edit mode, may take some minutes\")\n",
    "entity_dump_edit = EntityProfile.load_from_cache(load_dir=root_dir / \"entity_db\", edit_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7206525 VS 15202497\n"
     ]
    }
   ],
   "source": [
    "for alias in tqdm(aliases_to_remove):\n",
    "    for qid in list(entity_dump_edit.get_qid_cands(alias)):\n",
    "        entity_dump_edit.remove_mention(qid, alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved alias mapping at /dfs/scratch0/lorr1/projects/bootleg-data/data/wiki_title_0122/entity_db/entity_mappings/alias2qids_wikidata.json and id to /dfs/scratch0/lorr1/projects/bootleg-data/data/wiki_title_0122/entity_db/entity_mappings/alias2id_wikidata.json\n"
     ]
    }
   ],
   "source": [
    "new_dir = root_dir / 'entity_db_filt'\n",
    "entity_dump_edit.save(new_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
