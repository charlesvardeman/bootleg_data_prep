'''
python3 -m bootleg_data_prep.benchmarks.medmentions.build_med_dataset

Requires a concept dict from UMLS (see README.md) that looks like

"C0000005" : {
    "concept_id": "C0000005",
    "aliases": [
        "(131)I-MAA"
    ],
    "types": [
        "T116",
        "T121",
        "T130"
    ],
    "canonical_name": "(131)I-Macroaggregated Albumin"
},...

Dataset is in pubtator format
PMID | t | Title text
PMID | a | Abstract text
PMID TAB StartIndex TAB EndIndex TAB MentionTextSegment TAB SemanticTypeID TAB EntityID

EXAMPLE
25763772|t|DCTN4 as a modifier of chronic Pseudomonas aeruginosa infection in cystic fibrosis
25763772|a|Pseudomonas aeruginosa (Pa) infection in cystic fibrosis (CF) patients is associated with worse long-term pulmonary disease and shorter survival, and chronic Pa infection (CPA) is associated with reduced lung function, faster rate of lung decline, increased rates of exacerbations and shorter survival. By using exome sequencing and extreme phenotype design, it was recently shown that isoforms of dynactin 4 (DCTN4) may influence Pa infection in CF, leading to worse respiratory disease. The purpose of this study was to investigate the role of DCTN4 missense variants on Pa infection incidence, age at first Pa infection and chronic Pa infection incidence in a cohort of adult CF patients from a single centre. Polymerase chain reaction and direct sequencing were used to screen DNA samples for DCTN4 variants. A total of 121 adult CF patients from the Cochin Hospital CF centre have been included, all of them carrying two CFTR defects: 103 developed at least 1 pulmonary infection with Pa, and 68 patients of them had CPA. DCTN4 variants were identified in 24% (29/121) CF patients with Pa infection and in only 17% (3/18) CF patients with no Pa infection. Of the patients with CPA, 29% (20/68) had DCTN4 missense variants vs 23% (8/35) in patients without CPA. Interestingly, p.Tyr263Cys tend to be more frequently observed in CF patients with CPA than in patients without CPA (4/68 vs 0/35), and DCTN4 missense variants tend to be more frequent in male CF patients with CPA bearing two class II mutations than in male CF patients without CPA bearing two class II mutations (P = 0.06). Our observations reinforce that DCTN4 missense variants, especially p.Tyr263Cys, may be involved in the pathogenesis of CPA in male CF.
25763772        0       5       DCTN4   T116,T123       C4308010
...

Once prepared, the candidate lists can be generated by using scipyspacy.

First download the UMLS
'''
import glob
import os
from collections import defaultdict

import ujson as json
import argparse
from nltk.tokenize import word_tokenize
from tqdm import tqdm
import bootleg_data_prep.utils.data_prep_utils as prep_utils
from bootleg_data_prep.utils.classes.entity_symbols import EntitySymbols
from bootleg_data_prep.utils import utils

TITLE_KEY = "t"
ABSTRACT_KEY = "a"

class PubtatorObj:
    def __init__(self, doc_id, title, abstract, joint_text, entities):
        self.doc_id = int(doc_id)
        self.title = title
        self.abstract = abstract
        self.joint_text = joint_text
        self.entities = entities


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--in_dir', type=str, default='/dfs/scratch0/lorr1/MedMentions/full/data', help='Path to input file')
    parser.add_argument('--out_dir', type=str, default='/dfs/scratch0/lorr1/MedMentions/full/parsed_data', help='Path to input file')
    parser.add_argument('--concept_dict', type=str, default="/dfs/scratch0/lorr1/MedMentions/full/umls_concepts.json", help="See README for generation...requires UMLS and Scispacy")
    parser.add_argument('--data_file', type=str, default="corpus_pubtator.txt")
    parser.add_argument('--split_file_suffix', type=str, default="corpus_pubtator_pmids_")
    parser.add_argument('--overwrite_docs', action='store_true', help='Overwrite cached doc file')


    args = parser.parse_args()
    return args

def get_num_spaces(text):
    st_len = len(text)
    end_len = len(text.replace(" ", ""))
    return st_len - end_len

# Trying to get word offsets from character offsets ustil nltk tokenizer, which is a jerk to work with. The algorithm iterates over the characters
# (excluding spaces) and find the corresponding location in the nltk split sentence.
# Text_seg is the mention we are trying to extract that is in start_idx to end_idx
def convert_char_offsets_to_word(abstract, title, text_seg, start_idx, end_idx):
    # Med mentions determines the index by the joint text of title and abstract
    joint_text = f"{title} {abstract}"
    start_idx_orig = start_idx
    end_idx_orig = end_idx

    # Periods royally F up nltk word tokenizer at the end of the sentence, so I just remove them
    joint_text = joint_text.rstrip(".")
    if text_seg.endswith("."):
        end_idx -= 1
    text_seg = text_seg.rstrip(".")
    # Manual fix for if there might be indexing issues due to nltk word tokenizing making some single characters into doubles (e.g., '"')
    if '"' in joint_text:
        # print("start idx", start_idx, "end idx", end_idx)
        # print(joint_text[start_idx:end_idx])
        # print(joint_text)
        old_positions = [x for x, v in enumerate(joint_text) if v == '"']
        # print(old_positions)
        joint_text = joint_text.replace('"', '``')
        text_seg = text_seg.replace('"', '``')
        for p in old_positions:
            if p < start_idx_orig:
                start_idx += 1
            if p < end_idx_orig:
                end_idx += 1
            # print("p", p, "**" + joint_text[start_idx:end_idx] + "**", joint_text[start_idx:end_idx] == text_seg)
    orig_len = len(joint_text.replace(" ", ""))
    nltk_len = len(" ".join(word_tokenize(joint_text)).replace(" ", ""))
    if orig_len != nltk_len:
        print("Num characters between original and nltk joined are different. Add this as a manual fix rule")
    first_part = joint_text[:start_idx]
    middle_part = joint_text[start_idx:end_idx]
    up_to_end_part = joint_text[:end_idx]
    if middle_part.rstrip(".") != text_seg: # sometimes there is an off by 1 error
        start_idx += 1
        end_idx += 1
        first_part = joint_text[:start_idx]
        middle_part = joint_text[start_idx:end_idx]
        up_to_end_part = joint_text[:end_idx]
    assert middle_part.rstrip(".") == text_seg, f"**{middle_part}** does not equal **{text_seg}**"
    first_num_spaces = get_num_spaces(first_part)
    end_num_spaces = get_num_spaces(up_to_end_part)
    # We remove spaces from the character indexes because we iterate over words from nltk.word_tokenize
    new_start_idx = start_idx - first_num_spaces
    new_end_idx = end_idx - end_num_spaces
    # print("TEXT", joint_text)
    # print("SEG", text_seg)
    # print("start idx", start_idx, "end idx", end_idx, "new_start_idx", new_start_idx, "new_end_idx", new_end_idx)
    join_text_nltk = word_tokenize(joint_text)
    final_start_span = -1
    final_end_span = -1
    span = 0
    char_counter = 0
    # Get the word indexes for the mention
    for i, word in enumerate(join_text_nltk):
        # print("word", word, "span", span)
        for j, c in enumerate(word):
            # print("cahr", c, "char counter", char_counter, "new_start_idx", new_start_idx, "new_end_idx", new_end_idx)
            assert c != " "
            if char_counter == new_start_idx:
                final_start_span = span
            if char_counter == new_end_idx:
                final_end_span = span
                if c == "." and j == len(word)-1:
                    final_end_span += 1
            char_counter += 1
        span += 1
        if final_start_span != -1 and final_end_span != -1:
            break
    # If it's at the end, add one (which has already been done to span)
    if char_counter == new_end_idx:
        final_end_span = span
    # Happens when final_end_span is at the end of the sentence with some period (so the period will trigger final_end_span = span but span won't have been incremented)
    if final_end_span == final_start_span:
        final_end_span += 1
    # print("final start", final_start_span, "fina end", final_end_span)
    assert final_start_span != -1, f"final_start_span is -1"
    assert final_end_span != -1, f"final_end_span is -1"
    new_text_seg = " ".join(join_text_nltk[final_start_span:final_end_span])
    # assert new_text_seg.replace(" ", "") == text_seg.replace(" ", ""), f"**{new_text_seg}** and **{text_seg}** do not match"
    assert not (new_text_seg.replace(" ", "") != text_seg.replace(" ", "") and new_text_seg.rstrip(".").replace(" ", "") != text_seg.rstrip(".").replace(" ", ""))
    return final_start_span, final_end_span, new_text_seg.rstrip(".").strip()

def load_sentences(args):
    in_file = open(os.path.join(args.in_dir, args.data_file)).readlines()
    all_docs = []
    joint_text = None
    title = None
    abstract = None
    doc_id = None
    entities = []
    for line in tqdm(in_file):
        if len(line.strip()) <= 0:
            # We have ended one document
            all_docs.append(PubtatorObj(doc_id, title, abstract, joint_text, entities))
            joint_text = None
            title = None
            abstract = None
            doc_id = None
            entities = []
        # This is a title or abstract portion
        elif "|" in line:
            # print("HER", len(line.split("|", maxsplit=2)), line.split("|", maxsplit=2))
            pmid, key, text = line.split("|", maxsplit=2)
            text = text.strip()
            assert key in [TITLE_KEY, ABSTRACT_KEY], f"You have {key} that is not in {TITLE_KEY} or {ABSTRACT_KEY}"
            # We have started a new doc to save
            if key == TITLE_KEY:
                doc_id = pmid
                raw_title = text
                title = " ".join(word_tokenize(text))
            else: # key == ABSTRACT_KEY
                assert title is not None
                raw_abstract = text
                abstract = " ".join(word_tokenize(text))
                joint_text = " ".join(word_tokenize(f"{title} {abstract}"))
        else:
            assert "\t" in line, f"We are expecting a tab separator of {line}"
            assert abstract is not None and title is not None, f"Must have a title and text at this point"
            pmid, start_idx, end_idx, text_seg, type_id, cid = line.strip().split("\t")
            start_idx = int(start_idx)
            end_idx = int(end_idx)
            assert pmid == doc_id, f"We have a pmid {pmid} that does not equal doc_id {doc_id}"
            # We use nltk word_tokenize to process text; this will modify the spacings in the sentences
            start_span, end_span, proc_text_seg = convert_char_offsets_to_word(raw_abstract, raw_title, text_seg, start_idx, end_idx)
            entity = {
                "span": [start_span, end_span],
                "alias": proc_text_seg,
                "cid": cid,
                "typenames": type_id.split(",")
            }
            entities.append(entity)
    return all_docs

def load_data_splits(args):
    files = glob.glob(os.path.join(args.in_dir, f"{args.split_file_suffix}*"))
    print(f"Reading in {files} for data splits")
    splits = {}
    for file in files:
        in_f = open(file, "r")
        split_name = os.path.splitext(file)[0].split(args.split_file_suffix)[1]
        if split_name == "all":
            continue
        assert split_name in ["test", "dev", "trng"], f"{split_name} not in test, dev, or trng"
        if split_name == "trng":
            split_name = "train"
        split_ids = set()
        for line in in_f:
            split_ids.add(int(line.strip()))
        splits[split_name] = split_ids
        in_f.close()
    return splits

def gen_entity_dump(all_docs, concept_dict, train_ids):
    alias2qids_dict = defaultdict(lambda: defaultdict(int))
    qid2title = {}
    cid2qid = {}
    qid_counter = 1
    not_in_concepts = set()
    stats = defaultdict(int)
    for doc in all_docs:
        for entity in doc.entities:
            cid = entity["cid"].strip()
            # Only add alias/entity counts for things we've seen in training data
            other_alias = ""
            cid_aliases = []
            if int(doc.doc_id) in train_ids:
                other_alias = entity["alias"]
            stats["all_entities"] += 1
            if cid not in concept_dict:
                stats["not_found"] += 1
                not_in_concepts.add(cid)
                print(f"NOT FOUND {entity}")
                cid_title = cid
            else:
                stats["found"] += 1
                cid_aliases = concept_dict[cid]['aliases']
                if 'canonical_name' in concept_dict[cid]:
                    cid_title = concept_dict[cid]['canonical_name']
                elif len(cid_aliases) > 0:
                    print(f"Title not found for {entity}, using aliases")
                    cid_title = cid_aliases[0]
                else:
                    print(f"Title not found for {entity}, using cid")
                    cid_title = cid
            if cid in cid2qid:
                qid = cid2qid[cid]
            else:
                qid = f"Q{cid}"
                cid2qid[cid] = qid
                qid_counter += 1
            for al in cid_aliases:
                al = prep_utils.get_lnrm(al)
                alias2qids_dict[al][qid] += 1
            if other_alias != "":
                other_alias = prep_utils.get_lnrm(other_alias)
                alias2qids_dict[other_alias][qid] += 1
            if qid not in qid2title:
                qid2title[qid] = cid_title
            assert qid2title[qid] == cid_title, f"{qid2title[qid]} does not equal {cid_title} for qid {qid}"
    # This is legacy that we don't use anymore
    wpid2qid = {i+1:qid for i, qid in enumerate(qid2title.keys())}
    alias2qids = {}
    max_cands = 0
    max_alias_len = 0
    for alias in alias2qids_dict:
        # Legacy format that sucks
        arr = [[q, v] for q, v in alias2qids_dict[alias].items()]
        alias2qids[alias] = arr
        max_cands = max(max_cands, len(arr))
        max_alias_len = max(max_alias_len, len(alias.split(" ")))
    print(not_in_concepts)
    print(f"Found {len(alias2qids)} aliases, {len(qid2title)} qids, {len(not_in_concepts)} not in concept cids, {json.dumps(stats, indent=4)}")
    return max_cands, max_alias_len, alias2qids, qid2title, wpid2qid, cid2qid

def gen_type_dump(all_docs, cid2qid, concept_dict):
    qid2typeids = defaultdict(list)
    typename2typeid = {}
    type_id_counter = 0
    no_type = 0
    for doc in all_docs:
        for entity in doc.entities:
            cid = entity["cid"].strip()
            qid = cid2qid[cid]
            typenames = entity["typenames"]
            typeids = []
            for type in typenames:
                if type not in typename2typeid:
                    typename2typeid[type] = type_id_counter
                    type_id_counter += 1
                typeids.append(typename2typeid[type])
            if len(typeids) == 0:
                no_type += 1
            qid2typeids[qid] = typeids
    print(f"Extracted {type_id_counter} types. There are {no_type} out of {len(qid2typeids)} that do not have a type")
    return qid2typeids, typename2typeid

def generate_jsonl(doc, sent_id, cid2qid):
    qids = []
    aliases = []
    spans = []
    for entity in doc.entities:
        qids.append(cid2qid[entity["cid"].strip()])
        aliases.append(entity["alias"])
        spans.append("{:}:{:}".format(entity["span"][0], entity["span"][1]))
    data = {
        "sent_idx_unq": sent_id,
        "sentence": doc.joint_text,
        "parent_title": doc.title,
        "qids": qids,
        "aliases": aliases,
        "spans": spans,
        "gold": [True]*len(aliases)
    }
    return data

def write_data(args, all_docs, splits, cid2qid):
    out_files = {
        "dev": open(os.path.join(args.out_dir, "dev.jsonl"), "w"),
        "test": open(os.path.join(args.out_dir, "test.jsonl"), "w"),
        "train": open(os.path.join(args.out_dir, "train.jsonl"), "w"),
    }
    num_lines = 0
    total_aliases = 0
    split_lines = {"train": 0, "dev": 0, "test": 0}
    for doc in all_docs:
        if int(doc.doc_id) in splits["dev"]:
            out_file = out_files["dev"]
            split_lines["dev"] += 1
        elif int(doc.doc_id) in splits["test"]:
            out_file = out_files["test"]
            split_lines["test"] += 1
        else:
            out_file = out_files["train"]
            split_lines["train"] += 1
        jsonl_data = generate_jsonl(doc, num_lines, cid2qid)
        total_aliases += len(jsonl_data["aliases"])
        num_lines += 1
        out_file.write(json.dumps(jsonl_data) + "\n")
    print(f"Found {total_aliases} mentions. Wrote out {num_lines} total lines split into.")
    print(json.dumps(split_lines, indent=4))
    for k in out_files:
        out_files[k].close()

def main():
    args = parse_args()
    print(json.dumps(vars(args), indent=4))

    assert os.path.exists(args.concept_dict), f"You must provide a concept dict from UMLS...the file at {args.concept_dict} does not exist"

    concept_list = utils.load_json_file(args.concept_dict)
    concept_dict = {}
    for item in concept_list:
        concept_dict[item["concept_id"]] = item

    out_dir = args.out_dir
    utils.ensure_dir(out_dir)

    cache_file = os.path.join(out_dir, "_cahced_all_docs.pkl")
    if not os.path.exists(cache_file) or args.overwrite_docs:
        all_docs = load_sentences(args)
        utils.dump_pickle_file(cache_file, all_docs)
    else:
        all_docs = utils.load_pickle_file(cache_file)
    splits = load_data_splits(args)
    max_cands, max_alias_len, alias2qids, qid2title, wpid2qid, cid2qid = gen_entity_dump(all_docs, concept_dict, set(splits["train"]))
    entity_symbols_new = EntitySymbols(
        max_candidates=max_cands,
        alias2qids=alias2qids,
        qid2title=qid2title,
    )
    entity_symbols_new.save(os.path.join(out_dir, "entity_dump"))
    # Write our alias and qid count file
    aliasqid_count = {}
    for al in alias2qids:
        d = {}
        for pair in alias2qids[al]:
            d[pair[0]] = pair[1]
        aliasqid_count[al] = d
    utils.dump_json_file(os.path.join(args.out_dir, "med_mentions_alias_qid.json"), aliasqid_count)

    qid2typeids, typename2typeid = gen_type_dump(all_docs, cid2qid, concept_dict)
    out_f = open(os.path.join(out_dir, "med_mentions_qid2typeid.txt"), "w")
    for qid, typeids in qid2typeids.items():
        out_f.write(f"{qid},{'|'.join(map(str, typeids))}\n")
    out_f.close()
    utils.dump_json_file(filename=os.path.join(out_dir, "med_mentions_typename2typeid.json"), contents=typename2typeid)

    write_data(args, all_docs, splits, cid2qid)

    utils.dump_json_file(os.path.join(args.out_dir, "med_cid2qid.json"), cid2qid)


if __name__ == '__main__':
    main()
